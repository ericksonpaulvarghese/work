{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = \"/usr/java/jre1.8.0-x64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"joins_vsbroadcasting\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df = spark.read.csv('customers.txt', sep='\\t', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------------+--------------+----------------+\n",
      "|customer_id| customer_name|customer_city|customer_state|customer_zipcode|\n",
      "+-----------+--------------+-------------+--------------+----------------+\n",
      "|      11039|   Mary Torres|       Caguas|            PR|             725|\n",
      "|       5623|    Jose Haley|     Columbus|            OH|           43207|\n",
      "|       5829|    Mary Smith|      Houston|            TX|           77015|\n",
      "|       6336|Richard Maddox|       Caguas|            PR|             725|\n",
      "|       1708|Margaret Booth|    Arlington|            TX|           76010|\n",
      "+-----------+--------------+-------------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.read.csv(\"salestxns.tsv\", sep='\\t', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df1 = sales_df.toDF(\"TxnId\",\"CategoryId\",\"CategoryName\",\"ProductId\",\"ProductName\",\"Price\",\"Quantity\",\"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TxnId: integer (nullable = true)\n",
      " |-- CategoryId: integer (nullable = true)\n",
      " |-- CategoryName: string (nullable = true)\n",
      " |-- ProductId: integer (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- customer_zipcode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customised_schema = StructType([StructField(\"TxnID\", IntegerType(), True),\\\n",
    "# StructField(\"Category_ID\", IntegerType(), True]) \\ ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df1.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10240.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10485760/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10240.0/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50*1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [customer_id#59, TxnId#52, CategoryId#53, CategoryName#54, ProductId#55, ProductName#56, Price#57, Quantity#58, customer_name#13, customer_city#14, customer_state#15, customer_zipcode#16]\n",
      "+- SortMergeJoin [customer_id#59], [customer_id#12], LeftOuter\n",
      "   :- *Sort [customer_id#59 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(customer_id#59, 200)\n",
      "   :     +- *Project [_c0#35 AS TxnId#52, _c1#36 AS CategoryId#53, _c2#37 AS CategoryName#54, _c3#38 AS ProductId#55, _c4#39 AS ProductName#56, _c5#40 AS Price#57, _c6#41 AS Quantity#58, _c7#42 AS customer_id#59]\n",
      "   :        +- *FileScan csv [_c0#35,_c1#36,_c2#37,_c3#38,_c4#39,_c5#40,_c6#41,_c7#42] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/hduser/del_hyd/salestxns.tsv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:int,_c1:int,_c2:string,_c3:int,_c4:string,_c5:double,_c6:int,_c7:int>\n",
      "   +- *Sort [customer_id#12 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(customer_id#12, 200)\n",
      "         +- *FileScan csv [customer_id#12,customer_name#13,customer_city#14,customer_state#15,customer_zipcode#16] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/hduser/del_hyd/customers.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customer_id:int,customer_name:string,customer_city:string,customer_state:string,customer_z...\n",
      "Processing time taken :  0:00:17.394297\n"
     ]
    }
   ],
   "source": [
    "phys_exec_plan = sales_df1.join(cust_df, on = 'customer_id',how='left').explain()\n",
    "start_time = datetime.datetime.now()\n",
    "sales_df1.join(cust_df, on = 'customer_id',how='left').count()\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Processing time taken : \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",10485760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [customer_id#59, TxnId#52, CategoryId#53, CategoryName#54, ProductId#55, ProductName#56, Price#57, Quantity#58, customer_name#13, customer_city#14, customer_state#15, customer_zipcode#16]\n",
      "+- *BroadcastHashJoin [customer_id#59], [customer_id#12], LeftOuter, BuildRight\n",
      "   :- *Project [_c0#35 AS TxnId#52, _c1#36 AS CategoryId#53, _c2#37 AS CategoryName#54, _c3#38 AS ProductId#55, _c4#39 AS ProductName#56, _c5#40 AS Price#57, _c6#41 AS Quantity#58, _c7#42 AS customer_id#59]\n",
      "   :  +- *FileScan csv [_c0#35,_c1#36,_c2#37,_c3#38,_c4#39,_c5#40,_c6#41,_c7#42] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/hduser/del_hyd/salestxns.tsv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:int,_c1:int,_c2:string,_c3:int,_c4:string,_c5:double,_c6:int,_c7:int>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *FileScan csv [customer_id#12,customer_name#13,customer_city#14,customer_state#15,customer_zipcode#16] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/hduser/del_hyd/customers.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customer_id:int,customer_name:string,customer_city:string,customer_state:string,customer_z...\n",
      "Processing time taken :  0:00:01.472675\n"
     ]
    }
   ],
   "source": [
    "phys_exec_plan = sales_df1.join(broadcast(cust_df), on = 'customer_id',how='left').explain()\n",
    "start_time = datetime.datetime.now()\n",
    "sales_df1.join(broadcast(cust_df), on = 'customer_id',how='left').count()\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Processing time taken : \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_broadcast = 1.17\n",
    "with_broadcast = 0.11\n",
    "process_time_reduced = without_broadcast-with_broadcast\n",
    "pct = process_time_reduced/without_broadcast\n",
    "print(\"The process time reduced with the help of broadcasting is :\", pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
